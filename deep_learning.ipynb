{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:27:10.492064: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-22 11:27:10.492094: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.0\n",
      "Keras version: 2.4.0\n",
      "GPU is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:27:12.154938: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-11-22 11:27:12.155182: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-22 11:27:12.155196: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-22 11:27:12.155217: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-c8a454): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
    "    Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from midi2audio import FluidSynth\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_wav(midi_file, wav_file, soundfont_path):\n",
    "    fs = FluidSynth(soundfont_path)  # Load the soundfont\n",
    "    fs.midi_to_audio(midi_file, wav_file)  # Convert MIDI to WAV\n",
    "    print(f\"Converted {midi_file} to {wav_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'FluidR3_GM.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file 'maestro_dataset/mp3/sound_file_0.wav'..\n",
      "Converted maestro_dataset/midi/MIDI-Unprocessed_SMF_07_R1_2004_01_ORIG_MID--AUDIO_07_R1_2004_04_Track04_wav.midi to maestro_dataset/mp3/sound_file_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'FluidR3_GM.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file 'maestro_dataset/mp3/sound_file_1.wav'..\n",
      "Converted maestro_dataset/midi/MIDI-Unprocessed_XP_17_R2_2004_01_ORIG_MID--AUDIO_17_R2_2004_03_Track03_wav.midi to maestro_dataset/mp3/sound_file_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'FluidR3_GM.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file 'maestro_dataset/mp3/sound_file_2.wav'..\n",
      "Converted maestro_dataset/midi/MIDI-Unprocessed_SMF_17_R1_2004_01-02_ORIG_MID--AUDIO_20_R2_2004_04_Track04_wav.midi to maestro_dataset/mp3/sound_file_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'FluidR3_GM.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81762/1574302963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmidi_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mwav_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"sound_file_{sound_file_counter}.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmidi_to_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoundfont_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msound_file_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_81762/74502128.py\u001b[0m in \u001b[0;36mmidi_to_wav\u001b[0;34m(midi_file, wav_file, soundfont_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmidi_to_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoundfont_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFluidSynth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoundfont_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load the soundfont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidi_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_file\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert MIDI to WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Converted {midi_file} to {wav_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/midi2audio.py\u001b[0m in \u001b[0;36mmidi_to_audio\u001b[0;34m(self, midi_file, audio_file)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmidi_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fluidsynth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-ni'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msound_font\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sound_file_counter = 0\n",
    "soundfont_path = \"FluidR3_GM.sf2\"\n",
    "wav_path = \"maestro_dataset/mp3/\"\n",
    "\n",
    "for path, _ , file_names in os.walk('maestro_dataset/midi'):\n",
    "    for file_name in file_names:\n",
    "        midi_file = os.path.join(path, file_name)\n",
    "        wav_file = os.path.join(wav_path, f\"sound_file_{sound_file_counter}.wav\")\n",
    "        midi_to_wav(midi_file, wav_file, soundfont_path)\n",
    "\n",
    "        sound_file_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \"\"\"Loader is responsible for loading an audio file.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, duration, mono):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.mono = mono\n",
    "\n",
    "    def load(self, file_path):\n",
    "        signal = librosa.load(file_path,\n",
    "                              sr=self.sample_rate,\n",
    "                              duration=self.duration,\n",
    "                              mono=self.mono)[0]\n",
    "        return signal\n",
    "\n",
    "class Padder:\n",
    "    \"\"\"Padder is responsible to apply padding to an array.\"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"constant\"):\n",
    "        self.mode = mode\n",
    "\n",
    "    def left_pad(self, array, num_missing_items):\n",
    "        padded_array = np.pad(array,\n",
    "                              (num_missing_items, 0),\n",
    "                              mode=self.mode)\n",
    "        return padded_array\n",
    "\n",
    "    def right_pad(self, array, num_missing_items):\n",
    "        padded_array = np.pad(array,\n",
    "                              (0, num_missing_items),\n",
    "                              mode=self.mode)\n",
    "        return padded_array\n",
    "\n",
    "\n",
    "class LogSpectrogramExtractor:\n",
    "    \"\"\"LogSpectrogramExtractor extracts log spectrograms (in dB) from a\n",
    "    time-series signal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, frame_size, hop_length):\n",
    "        self.frame_size = frame_size\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def extract(self, signal):\n",
    "        stft = librosa.stft(signal,\n",
    "                            n_fft=self.frame_size,\n",
    "                            hop_length=self.hop_length)[:-1]\n",
    "        spectrogram = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "        return log_spectrogram\n",
    "\n",
    "\n",
    "class MinMaxNormaliser:\n",
    "    \"\"\"MinMaxNormaliser applies min max normalisation to an array.\"\"\"\n",
    "\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min = min_val\n",
    "        self.max = max_val\n",
    "\n",
    "    def normalise(self, array):\n",
    "        norm_array = (array - array.min()) / (array.max() - array.min())\n",
    "        norm_array = norm_array * (self.max - self.min) + self.min\n",
    "        return norm_array\n",
    "\n",
    "    def denormalise(self, norm_array, original_min, original_max):\n",
    "        array = (norm_array - self.min) / (self.max - self.min)\n",
    "        array = array * (original_max - original_min) + original_min\n",
    "        return array\n",
    "\n",
    "\n",
    "class Saver:\n",
    "    \"\"\"saver is responsible to save features, and the min max values.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_save_dir, min_max_values_save_dir):\n",
    "        self.feature_save_dir = feature_save_dir\n",
    "        self.min_max_values_save_dir = min_max_values_save_dir\n",
    "\n",
    "    def save_feature(self, feature, file_path):\n",
    "        save_path = self._generate_save_path(file_path)\n",
    "        np.save(save_path, feature)\n",
    "        return save_path\n",
    "\n",
    "    def save_min_max_values(self, min_max_values):\n",
    "        save_path = os.path.join(self.min_max_values_save_dir,\n",
    "                                 \"min_max_values.pkl\")\n",
    "        self._save(min_max_values, save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save(data, save_path):\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _generate_save_path(self, file_path):\n",
    "        file_name = os.path.split(file_path)[1]\n",
    "        save_path = os.path.join(self.feature_save_dir, file_name + \".npy\")\n",
    "        return save_path\n",
    "\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"PreprocessingPipeline processes audio files in a directory, applying\n",
    "    the following steps to each file:\n",
    "        1- load a file\n",
    "        2- pad the signal (if necessary)\n",
    "        3- extracting log spectrogram from signal\n",
    "        4- normalise spectrogram\n",
    "        5- save the normalised spectrogram\n",
    "\n",
    "    Storing the min max values for all the log spectrograms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.padder = None\n",
    "        self.extractor = None\n",
    "        self.normaliser = None\n",
    "        self.saver = None\n",
    "        self.min_max_values = {}\n",
    "        self._loader = None\n",
    "        self._num_expected_samples = None\n",
    "\n",
    "    @property\n",
    "    def loader(self):\n",
    "        return self._loader\n",
    "\n",
    "    @loader.setter\n",
    "    def loader(self, loader):\n",
    "        self._loader = loader\n",
    "        self._num_expected_samples = int(loader.sample_rate * loader.duration)\n",
    "\n",
    "    def process(self, audio_files_dir):\n",
    "        for root, _, files in os.walk(audio_files_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                self._process_file(file_path)\n",
    "                print(f\"Processed file {file_path}\")\n",
    "        self.saver.save_min_max_values(self.min_max_values)\n",
    "\n",
    "    def _process_file(self, file_path):\n",
    "        signal = self.loader.load(file_path)\n",
    "        if self._is_padding_necessary(signal):\n",
    "            signal = self._apply_padding(signal)\n",
    "        feature = self.extractor.extract(signal)\n",
    "        norm_feature = self.normaliser.normalise(feature)\n",
    "        save_path = self.saver.save_feature(norm_feature, file_path)\n",
    "        self._store_min_max_value(save_path, feature.min(), feature.max())\n",
    "\n",
    "    def _is_padding_necessary(self, signal):\n",
    "        if len(signal) < self._num_expected_samples:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _apply_padding(self, signal):\n",
    "        num_missing_samples = self._num_expected_samples - len(signal)\n",
    "        padded_signal = self.padder.right_pad(signal, num_missing_samples)\n",
    "        return padded_signal\n",
    "\n",
    "    def _store_min_max_value(self, save_path, min_val, max_val):\n",
    "        self.min_max_values[save_path] = {\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file maestro_dataset/mp3/sound_file_21.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_14.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_27.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_15.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_17.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_8.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_6.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_19.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_20.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_13.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_23.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_3.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_33.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_12.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_5.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_0.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_10.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_31.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_16.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_9.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_18.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_22.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_11.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_1.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_2.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_24.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_28.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_25.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_7.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_4.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_30.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_26.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_29.wav\n",
      "Processed file maestro_dataset/mp3/sound_file_32.wav\n"
     ]
    }
   ],
   "source": [
    "# FRAME_SIZE = 512\n",
    "# HOP_LENGTH = 256\n",
    "# DURATION = 30.0  # in seconds\n",
    "# SAMPLE_RATE = 22050\n",
    "# MONO = True\n",
    "\n",
    "# SPECTROGRAMS_SAVE_DIR = \"maestro_dataset/spectograms\"\n",
    "# MIN_MAX_VALUES_SAVE_DIR = \"maestro_dataset/minmax\"\n",
    "# FILES_DIR = \"maestro_dataset/mp3\"\n",
    "\n",
    "# # instantiate all objects\n",
    "# loader = Loader(SAMPLE_RATE, DURATION, MONO)\n",
    "# padder = Padder()\n",
    "# log_spectrogram_extractor = LogSpectrogramExtractor(FRAME_SIZE, HOP_LENGTH)\n",
    "# min_max_normaliser = MinMaxNormaliser(0, 1)\n",
    "# saver = Saver(SPECTROGRAMS_SAVE_DIR, MIN_MAX_VALUES_SAVE_DIR)\n",
    "\n",
    "# preprocessing_pipeline = PreprocessingPipeline()\n",
    "# preprocessing_pipeline.loader = loader\n",
    "# preprocessing_pipeline.padder = padder\n",
    "# preprocessing_pipeline.extractor = log_spectrogram_extractor\n",
    "# preprocessing_pipeline.normaliser = min_max_normaliser\n",
    "# preprocessing_pipeline.saver = saver\n",
    "\n",
    "# preprocessing_pipeline.process(FILES_DIR)\n",
    "\n",
    "FRAME_SIZE = 512\n",
    "HOP_LENGTH = 256\n",
    "DURATION = 4.5  # in seconds\n",
    "SAMPLE_RATE = 22050\n",
    "MONO = True\n",
    "\n",
    "SPECTROGRAMS_SAVE_DIR = \"maestro_dataset/spectograms\"\n",
    "MIN_MAX_VALUES_SAVE_DIR = \"maestro_dataset/minmax\"\n",
    "FILES_DIR = \"maestro_dataset/mp3\"\n",
    "\n",
    "# instantiate all objects\n",
    "loader = Loader(SAMPLE_RATE, DURATION, MONO)\n",
    "padder = Padder()\n",
    "log_spectrogram_extractor = LogSpectrogramExtractor(FRAME_SIZE, HOP_LENGTH)\n",
    "min_max_normaliser = MinMaxNormaliser(0, 1)\n",
    "saver = Saver(SPECTROGRAMS_SAVE_DIR, MIN_MAX_VALUES_SAVE_DIR)\n",
    "\n",
    "preprocessing_pipeline = PreprocessingPipeline()\n",
    "preprocessing_pipeline.loader = loader\n",
    "preprocessing_pipeline.padder = padder\n",
    "preprocessing_pipeline.extractor = log_spectrogram_extractor\n",
    "preprocessing_pipeline.normaliser = min_max_normaliser\n",
    "preprocessing_pipeline.saver = saver\n",
    "\n",
    "preprocessing_pipeline.process(FILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_reconstruction_loss(y_target, y_predicted):\n",
    "    error = y_target - y_predicted\n",
    "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def calculate_kl_loss(model):\n",
    "    # wrap `_calculate_kl_loss` such that it takes the model as an argument,\n",
    "    # returns a function which can take arbitrary number of arguments\n",
    "    # (for compatibility with `metrics` and utility in the loss function)\n",
    "    # and returns the kl loss\n",
    "    def _calculate_kl_loss(*args):\n",
    "        kl_loss = -0.5 * K.sum(1 + model.log_variance - K.square(model.mu) -\n",
    "                               K.exp(model.log_variance), axis=1)\n",
    "        return kl_loss\n",
    "    return _calculate_kl_loss\n",
    "\n",
    "\n",
    "class VAE:\n",
    "    \"\"\"\n",
    "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
    "    with mirrored encoder and decoder components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        self.input_shape = input_shape # [28, 28, 1]\n",
    "        self.conv_filters = conv_filters # [2, 4, 8]\n",
    "        self.conv_kernels = conv_kernels # [3, 5, 3]\n",
    "        self.conv_strides = conv_strides # [1, 2, 2]\n",
    "        self.latent_space_dim = latent_space_dim # 2\n",
    "        self.reconstruction_loss_weight = 1000\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.model = None\n",
    "\n",
    "        self._num_conv_layers = len(conv_filters)\n",
    "        self._shape_before_bottleneck = None\n",
    "        self._model_input = None\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        self.model.summary()\n",
    "\n",
    "    def compile(self, learning_rate=0.0001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss=self._calculate_combined_loss,\n",
    "                           metrics=[_calculate_reconstruction_loss,\n",
    "                                    calculate_kl_loss(self)])\n",
    "\n",
    "    def train(self, x_train, batch_size, num_epochs):\n",
    "        self.model.fit(x_train,\n",
    "                       x_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=num_epochs,\n",
    "                       shuffle=True)\n",
    "\n",
    "    def save(self, save_folder=\".\"):\n",
    "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
    "        self._save_parameters(save_folder)\n",
    "        self._save_weights(save_folder)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def reconstruct(self, images):\n",
    "        latent_representations = self.encoder.predict(images)\n",
    "        reconstructed_images = self.decoder.predict(latent_representations)\n",
    "        return reconstructed_images, latent_representations\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_folder=\".\"):\n",
    "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        autoencoder = VAE(*parameters)\n",
    "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
    "        autoencoder.load_weights(weights_path)\n",
    "        return autoencoder\n",
    "\n",
    "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
    "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
    "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
    "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
    "                                                         + kl_loss\n",
    "        return combined_loss\n",
    "\n",
    "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
    "        error = y_target - y_predicted\n",
    "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
    "        return reconstruction_loss\n",
    "\n",
    "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
    "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
    "                               K.exp(self.log_variance), axis=1)\n",
    "        return kl_loss\n",
    "\n",
    "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "    def _save_parameters(self, save_folder):\n",
    "        parameters = [\n",
    "            self.input_shape,\n",
    "            self.conv_filters,\n",
    "            self.conv_kernels,\n",
    "            self.conv_strides,\n",
    "            self.latent_space_dim\n",
    "        ]\n",
    "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "    def _save_weights(self, save_folder):\n",
    "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
    "        self.model.save_weights(save_path)\n",
    "\n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        model_input = self._model_input\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        decoder_input = self._add_decoder_input()\n",
    "        dense_layer = self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "\n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
    "\n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
    "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
    "        return dense_layer\n",
    "\n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "\n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        \"\"\"Add conv transpose blocks.\"\"\"\n",
    "        # loop through all the conv layers in reverse order and stop at the\n",
    "        # first layer\n",
    "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
    "            x = self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "\n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
    "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
    "        return x\n",
    "\n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=self.conv_kernels[0],\n",
    "            strides=self.conv_strides[0],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
    "        return output_layer\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()\n",
    "        conv_layers = self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self._model_input = encoder_input\n",
    "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
    "\n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
    "\n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
    "        x = encoder_input\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)\n",
    "        return x\n",
    "\n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
    "        conv 2d + ReLU + batch normalization.\n",
    "        \"\"\"\n",
    "        layer_number = layer_index + 1\n",
    "        conv_layer = Conv2D(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"encoder_conv_layer_{layer_number}\"\n",
    "        )\n",
    "        x = conv_layer(x)\n",
    "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
    "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
    "        return x\n",
    "\n",
    "    def _add_bottleneck(self, x):\n",
    "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
    "        layer).\n",
    "        \"\"\"\n",
    "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
    "        x = Flatten()(x)\n",
    "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
    "        self.log_variance = Dense(self.latent_space_dim,\n",
    "                                  name=\"log_variance\")(x)\n",
    "\n",
    "        def sample_point_from_normal_distribution(args):\n",
    "            mu, log_variance = args\n",
    "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
    "                                      stddev=1.)\n",
    "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
    "            return sampled_point\n",
    "\n",
    "        print(self.input_shape)\n",
    "\n",
    "        x = Lambda(sample_point_from_normal_distribution,\n",
    "               name=\"encoder_output\")([self.mu, self.log_variance])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 150\n",
    "\n",
    "SPECTROGRAMS_PATH = \"maestro_dataset/spectograms\"\n",
    "\n",
    "\n",
    "def load_fsdd(spectrograms_path):\n",
    "    x_train = []\n",
    "    for root, _, file_names in os.walk(spectrograms_path):\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            spectrogram = np.load(file_path) # (n_bins, n_frames, 1)\n",
    "            x_train.append(spectrogram)\n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train[..., np.newaxis] # -> (3000, 256, 64, 1)\n",
    "    return x_train\n",
    "\n",
    "\n",
    "def train(x_train, learning_rate, batch_size, epochs):\n",
    "    autoencoder = VAE(\n",
    "        input_shape=(256, 388, 1),\n",
    "        conv_filters=(512, 256, 128, 64, 32),\n",
    "        conv_kernels=(3, 3, 3, 3, 3),\n",
    "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
    "        latent_space_dim=128\n",
    "    )\n",
    "    autoencoder.summary()\n",
    "    autoencoder.compile(learning_rate)\n",
    "    autoencoder.train(x_train, batch_size, epochs)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 345, 1)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 256, 345, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_1 (Conv2D)   (None, 128, 173, 512 5120        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_1 (ReLU)           (None, 128, 173, 512 0           encoder_conv_layer_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_1 (BatchNormalizatio (None, 128, 173, 512 2048        encoder_relu_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_2 (Conv2D)   (None, 64, 87, 256)  1179904     encoder_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_2 (ReLU)           (None, 64, 87, 256)  0           encoder_conv_layer_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_2 (BatchNormalizatio (None, 64, 87, 256)  1024        encoder_relu_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_3 (Conv2D)   (None, 32, 44, 128)  295040      encoder_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_3 (ReLU)           (None, 32, 44, 128)  0           encoder_conv_layer_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_3 (BatchNormalizatio (None, 32, 44, 128)  512         encoder_relu_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_4 (Conv2D)   (None, 16, 22, 64)   73792       encoder_bn_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_4 (ReLU)           (None, 16, 22, 64)   0           encoder_conv_layer_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_4 (BatchNormalizatio (None, 16, 22, 64)   256         encoder_relu_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_5 (Conv2D)   (None, 8, 22, 32)    18464       encoder_bn_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_5 (ReLU)           (None, 8, 22, 32)    0           encoder_conv_layer_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_5 (BatchNormalizatio (None, 8, 22, 32)    128         encoder_relu_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5632)         0           encoder_bn_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 128)          721024      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "log_variance (Dense)            (None, 128)          721024      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 128)          0           mu[0][0]                         \n",
      "                                                                 log_variance[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,018,336\n",
      "Trainable params: 3,016,352\n",
      "Non-trainable params: 1,984\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_dense (Dense)        (None, 5632)              726528    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 8, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 16, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "decoder_relu_1 (ReLU)        (None, 16, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_1 (BatchNormaliza (None, 16, 22, 32)        128       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 32, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "decoder_relu_2 (ReLU)        (None, 32, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_2 (BatchNormaliza (None, 32, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 64, 88, 128)       73856     \n",
      "_________________________________________________________________\n",
      "decoder_relu_3 (ReLU)        (None, 64, 88, 128)       0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_3 (BatchNormaliza (None, 64, 88, 128)       512       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 128, 176, 256)     295168    \n",
      "_________________________________________________________________\n",
      "decoder_relu_4 (ReLU)        (None, 128, 176, 256)     0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_4 (BatchNormaliza (None, 128, 176, 256)     1024      \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 256, 352, 1)       2305      \n",
      "_________________________________________________________________\n",
      "sigmoid_layer (Activation)   (None, 256, 352, 1)       0         \n",
      "=================================================================\n",
      "Total params: 1,127,521\n",
      "Trainable params: 1,126,561\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 256, 345, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 128)               3018336   \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 256, 352, 1)       1127521   \n",
      "=================================================================\n",
      "Total params: 4,145,857\n",
      "Trainable params: 4,142,913\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected encoder_input to have shape (256, 345, 1) but got array with shape (256, 388, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85567/1717284402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fsdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPECTROGRAMS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_85567/1273805679.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_85567/2596854517.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                        shuffle=True)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2361\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    538\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    541\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected encoder_input to have shape (256, 345, 1) but got array with shape (256, 388, 1)"
     ]
    }
   ],
   "source": [
    "x_train = load_fsdd(SPECTROGRAMS_PATH)\n",
    "autoencoder = train(x_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
    "autoencoder.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundGenerator:\n",
    "    \"\"\"SoundGenerator is responsible for generating audios from\n",
    "    spectrograms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vae, hop_length):\n",
    "        self.vae = vae\n",
    "        self.hop_length = hop_length\n",
    "        self._min_max_normaliser = MinMaxNormaliser(0, 1)\n",
    "\n",
    "    def generate(self, spectrograms, min_max_values):\n",
    "        generated_spectrograms, latent_representations = \\\n",
    "            self.vae.reconstruct(spectrograms)\n",
    "        signals = self.convert_spectrograms_to_audio(generated_spectrograms, min_max_values)\n",
    "        return signals, latent_representations\n",
    "\n",
    "    def convert_spectrograms_to_audio(self, spectrograms, min_max_values):\n",
    "        signals = []\n",
    "        for spectrogram, min_max_value in zip(spectrograms, min_max_values):\n",
    "            # reshape the log spectrogram\n",
    "            log_spectrogram = spectrogram[:, :, 0]\n",
    "            # apply denormalisation\n",
    "            denorm_log_spec = self._min_max_normaliser.denormalise(\n",
    "                log_spectrogram, min_max_value[\"min\"], min_max_value[\"max\"])\n",
    "            # log spectrogram -> spectrogram\n",
    "            spec = librosa.db_to_amplitude(denorm_log_spec)\n",
    "            # apply Griffin-Lim\n",
    "            signal = librosa.istft(spec, hop_length=self.hop_length)\n",
    "            # append signal to \"signals\"\n",
    "            signals.append(signal)\n",
    "        return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "HOP_LENGTH = 256\n",
    "SAVE_DIR_ORIGINAL = \"samples/original/\"\n",
    "SAVE_DIR_GENERATED = \"samples/generated/\"\n",
    "MIN_MAX_VALUES_PATH = \"maestro_dataset/minmax/min_max_values.pkl\"\n",
    "\n",
    "\n",
    "def load_fsdd(spectrograms_path):\n",
    "    x_train = []\n",
    "    file_paths = []\n",
    "    print(spectrograms_path)\n",
    "    for root, _, file_names in os.walk(spectrograms_path):\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            spectrogram = np.load(file_path) # (n_bins, n_frames, 1)\n",
    "            x_train.append(spectrogram)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train[..., np.newaxis] # -> (3000, 256, 64, 1)\n",
    "    return x_train, file_paths\n",
    "\n",
    "\n",
    "def select_spectrograms(spectrograms,\n",
    "                        file_paths,\n",
    "                        min_max_values,\n",
    "                        num_spectrograms=2):\n",
    "    sampled_indexes = np.random.choice(range(len(spectrograms)), num_spectrograms)\n",
    "    sampled_spectrogrmas = spectrograms[sampled_indexes]\n",
    "    file_paths = [file_paths[index] for index in sampled_indexes]\n",
    "    print(file_paths)\n",
    "    print(min_max_values)\n",
    "    sampled_min_max_values = [min_max_values[file_path] for file_path in\n",
    "                           file_paths]\n",
    "    print(file_paths)\n",
    "    print(sampled_min_max_values)\n",
    "    return sampled_spectrogrmas, sampled_min_max_values\n",
    "\n",
    "\n",
    "def save_signals(signals, save_dir, sample_rate=22050):\n",
    "    for i, signal in enumerate(signals):\n",
    "        save_path = os.path.join(save_dir, str(i) + \".wav\")\n",
    "        sf.write(save_path, signal, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 1)\n",
      "{'maestro_dataset/spectograms/sound_file_21.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_14.wav.npy': {'min': -86.20711, 'max': -6.2071075}, 'maestro_dataset/spectograms/sound_file_27.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_15.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_17.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_8.wav.npy': {'min': -81.74089, 'max': -1.7408905}, 'maestro_dataset/spectograms/sound_file_6.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_19.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_20.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_13.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_23.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_3.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_33.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_12.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_5.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_0.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_10.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_31.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_16.wav.npy': {'min': -84.19915, 'max': -4.1991515}, 'maestro_dataset/spectograms/sound_file_9.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_18.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_22.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_11.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_1.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_2.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_24.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_28.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_25.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_7.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_4.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_30.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_26.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_29.wav.npy': {'min': -78.06798, 'max': 1.9320221}, 'maestro_dataset/spectograms/sound_file_32.wav.npy': {'min': -100.0, 'max': -42.1442}}\n",
      "maestro_dataset/spectograms\n",
      "['maestro_dataset/spectograms/sound_file_7.wav.npy', 'maestro_dataset/spectograms/sound_file_13.wav.npy', 'maestro_dataset/spectograms/sound_file_8.wav.npy', 'maestro_dataset/spectograms/sound_file_5.wav.npy', 'maestro_dataset/spectograms/sound_file_23.wav.npy']\n",
      "{'maestro_dataset/spectograms/sound_file_21.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_14.wav.npy': {'min': -86.20711, 'max': -6.2071075}, 'maestro_dataset/spectograms/sound_file_27.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_15.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_17.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_8.wav.npy': {'min': -81.74089, 'max': -1.7408905}, 'maestro_dataset/spectograms/sound_file_6.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_19.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_20.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_13.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_23.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_3.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_33.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_12.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_5.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_0.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_10.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_31.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_16.wav.npy': {'min': -84.19915, 'max': -4.1991515}, 'maestro_dataset/spectograms/sound_file_9.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_18.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_22.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_11.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_1.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_2.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_24.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_28.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_25.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_7.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_4.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_30.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_26.wav.npy': {'min': -100.0, 'max': -42.1442}, 'maestro_dataset/spectograms/sound_file_29.wav.npy': {'min': -78.06798, 'max': 1.9320221}, 'maestro_dataset/spectograms/sound_file_32.wav.npy': {'min': -100.0, 'max': -42.1442}}\n",
      "['maestro_dataset/spectograms/sound_file_7.wav.npy', 'maestro_dataset/spectograms/sound_file_13.wav.npy', 'maestro_dataset/spectograms/sound_file_8.wav.npy', 'maestro_dataset/spectograms/sound_file_5.wav.npy', 'maestro_dataset/spectograms/sound_file_23.wav.npy']\n",
      "[{'min': -100.0, 'max': -42.1442}, {'min': -100.0, 'max': -42.1442}, {'min': -81.74089, 'max': -1.7408905}, {'min': -100.0, 'max': -42.1442}, {'min': -100.0, 'max': -42.1442}]\n"
     ]
    }
   ],
   "source": [
    "# initialise sound generator\n",
    "vae = VAE.load(\"model\")\n",
    "sound_generator = SoundGenerator(vae, HOP_LENGTH)\n",
    "\n",
    "# load spectrograms + min max values\n",
    "with open(MIN_MAX_VALUES_PATH, \"rb\") as f:\n",
    "    min_max_values = pickle.load(f)\n",
    "    print(min_max_values)\n",
    "    \n",
    "specs, file_paths = load_fsdd(SPECTROGRAMS_PATH)\n",
    "\n",
    "# sample spectrograms + min max values\n",
    "sampled_specs, sampled_min_max_values = select_spectrograms(specs,\n",
    "                                                            file_paths,\n",
    "                                                            min_max_values,\n",
    "                                                            5)\n",
    "\n",
    "\n",
    "# generate audio for sampled spectrograms\n",
    "signals, _ = sound_generator.generate(sampled_specs,\n",
    "                                      sampled_min_max_values)\n",
    "\n",
    "# convert spectrogram samples to audio\n",
    "original_signals = sound_generator.convert_spectrograms_to_audio(\n",
    "    sampled_specs, sampled_min_max_values)\n",
    "\n",
    "# save audio signals\n",
    "save_signals(signals, SAVE_DIR_GENERATED)\n",
    "save_signals(original_signals, SAVE_DIR_ORIGINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
